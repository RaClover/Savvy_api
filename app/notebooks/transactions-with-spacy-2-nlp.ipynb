{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7598325,"sourceType":"datasetVersion","datasetId":4423015}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # Install Necessary Packages\n# !pip install -U spacy\n# !pip install spacy-lookups-data","metadata":{"execution":{"iopub.status.busy":"2024-06-07T01:48:07.865324Z","iopub.execute_input":"2024-06-07T01:48:07.866044Z","iopub.status.idle":"2024-06-07T01:48:07.870476Z","shell.execute_reply.started":"2024-06-07T01:48:07.866014Z","shell.execute_reply":"2024-06-07T01:48:07.869332Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport spacy\nfrom spacy.training import Example\nimport random\nimport json\nfrom spacy.util import minibatch, compounding\nfrom spacy.scorer import Scorer\nimport warnings\n# Suppress UserWarning from spaCy\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n# Load dataset\ndf = pd.read_csv('/kaggle/input/conll-transactions/annotated_transactions.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T01:48:07.874422Z","iopub.execute_input":"2024-06-07T01:48:07.874737Z","iopub.status.idle":"2024-06-07T01:48:17.097574Z","shell.execute_reply.started":"2024-06-07T01:48:07.874711Z","shell.execute_reply":"2024-06-07T01:48:17.096704Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Function to remove overlapping entities\ndef remove_overlaps(entities):\n    sorted_entities = sorted(entities, key=lambda e: e[0])\n    non_overlapping_entities = []\n    last_end = -1\n    for start, end, label in sorted_entities:\n        if start >= last_end:\n            non_overlapping_entities.append((start, end, label))\n            last_end = end\n    return non_overlapping_entities","metadata":{"execution":{"iopub.status.busy":"2024-06-07T01:48:17.099254Z","iopub.execute_input":"2024-06-07T01:48:17.099575Z","iopub.status.idle":"2024-06-07T01:48:17.105505Z","shell.execute_reply.started":"2024-06-07T01:48:17.099547Z","shell.execute_reply":"2024-06-07T01:48:17.104403Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Function to parse annotations and convert them to spaCy format\ndef parse_annotations(text, annotation_json):\n    entities = []\n    annotations = json.loads(annotation_json)\n    for annotation in annotations:\n        start = annotation['start']\n        end = annotation['end']\n        label = annotation['labels'][0]\n        entities.append((start, end, label))\n    return (text, {\"entities\": remove_overlaps(entities)})","metadata":{"execution":{"iopub.status.busy":"2024-06-07T01:48:17.106723Z","iopub.execute_input":"2024-06-07T01:48:17.107448Z","iopub.status.idle":"2024-06-07T01:48:17.115768Z","shell.execute_reply.started":"2024-06-07T01:48:17.107423Z","shell.execute_reply":"2024-06-07T01:48:17.114865Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Split the dataset\ntrain_indices = df.sample(frac=0.8, random_state=42).index\ntest_indices = df.index.difference(train_indices)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T01:48:17.118410Z","iopub.execute_input":"2024-06-07T01:48:17.118667Z","iopub.status.idle":"2024-06-07T01:48:17.131985Z","shell.execute_reply.started":"2024-06-07T01:48:17.118644Z","shell.execute_reply":"2024-06-07T01:48:17.131141Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data = [parse_annotations(row['text'], row['label']) for _, row in df.loc[train_indices].iterrows()]\ntest_data = [parse_annotations(row['text'], row['label']) for _, row in df.loc[test_indices].iterrows()]","metadata":{"execution":{"iopub.status.busy":"2024-06-07T01:48:17.133184Z","iopub.execute_input":"2024-06-07T01:48:17.133500Z","iopub.status.idle":"2024-06-07T01:48:17.403768Z","shell.execute_reply.started":"2024-06-07T01:48:17.133463Z","shell.execute_reply":"2024-06-07T01:48:17.402957Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Define the hyperparameter search space\nhyperparameter_space = {\n    'learning_rate': [0.001],\n    'dropout': [0.5],\n    'batch_size': [32],\n    'epochs': [100]\n}\n\nbest_model = None\nbest_score = 0.0","metadata":{"execution":{"iopub.status.busy":"2024-06-07T01:48:17.405156Z","iopub.execute_input":"2024-06-07T01:48:17.405864Z","iopub.status.idle":"2024-06-07T01:48:17.410736Z","shell.execute_reply.started":"2024-06-07T01:48:17.405830Z","shell.execute_reply":"2024-06-07T01:48:17.409776Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Random search\nfor _ in range(4):  \n    # Randomly sample hyperparameters from the search space\n    hyperparameters = {\n        'learning_rate': random.choice(hyperparameter_space['learning_rate']),\n        'dropout': random.choice(hyperparameter_space['dropout']),\n        'batch_size': random.choice(hyperparameter_space['batch_size']),\n        'epochs': random.choice(hyperparameter_space['epochs'])\n    }\n\n    # Initialize blank English model\n    nlp = spacy.blank(\"en\")\n\n    # Add the NER pipeline\n    if 'ner' not in nlp.pipe_names:\n        ner = nlp.add_pipe(\"ner\", last=True)  # Add NER to the pipeline\n\n    # Add entity labels to NER\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])  # Add labels\n\n    #  initialize the NER model before training\n    nlp.initialize()\n\n    # Training the NER model\n    optimizer = nlp.begin_training()\n\n    # Iterate over epochs\n    for itn in range(hyperparameters['epochs']):\n        random.shuffle(train_data)\n        losses = {}\n\n        # Batch up the examples using spaCy's minibatch\n        batches = minibatch(train_data, size=compounding(4., hyperparameters['batch_size'], 1.001))\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            example = []\n            # Update the model with each example\n            for i in range(len(texts)):\n                doc = nlp.make_doc(texts[i])\n                example.append(Example.from_dict(doc, annotations[i]))\n            nlp.update(example, drop=hyperparameters['dropout'], losses=losses)\n\n        # Print the loss for each iteration\n        print(f\"Iteration {itn}, Losses: {losses}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T01:48:17.411952Z","iopub.execute_input":"2024-06-07T01:48:17.412554Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"[2024-06-07 01:48:17,897] [INFO] Created vocabulary\n[2024-06-07 01:48:17,899] [INFO] Finished initializing nlp object\n[2024-06-07 01:48:17,985] [INFO] Created vocabulary\n[2024-06-07 01:48:17,986] [INFO] Finished initializing nlp object\n","output_type":"stream"},{"name":"stdout","text":"Iteration 0, Losses: {'ner': 3512.5801997977933}\nIteration 1, Losses: {'ner': 688.6714811632727}\nIteration 2, Losses: {'ner': 434.3008688483717}\nIteration 3, Losses: {'ner': 340.0586168058623}\nIteration 4, Losses: {'ner': 338.0330282848331}\nIteration 5, Losses: {'ner': 250.49893272804792}\nIteration 6, Losses: {'ner': 335.59012388698903}\nIteration 7, Losses: {'ner': 285.81241904914737}\nIteration 8, Losses: {'ner': 294.02123249997715}\nIteration 9, Losses: {'ner': 228.4792640380576}\nIteration 10, Losses: {'ner': 228.9052336738424}\nIteration 11, Losses: {'ner': 189.48829320832868}\nIteration 12, Losses: {'ner': 175.1593551433526}\nIteration 13, Losses: {'ner': 174.14461304779414}\nIteration 14, Losses: {'ner': 171.92541353640726}\nIteration 15, Losses: {'ner': 182.26094976526963}\nIteration 16, Losses: {'ner': 181.11814364237904}\nIteration 17, Losses: {'ner': 198.74080199235468}\nIteration 18, Losses: {'ner': 156.45967554244535}\nIteration 19, Losses: {'ner': 151.0060174244344}\nIteration 20, Losses: {'ner': 156.35146249182912}\nIteration 21, Losses: {'ner': 155.06180674099477}\nIteration 22, Losses: {'ner': 129.70426749197517}\nIteration 23, Losses: {'ner': 154.90658775671454}\nIteration 24, Losses: {'ner': 116.35709887530182}\nIteration 25, Losses: {'ner': 157.7083369840809}\nIteration 26, Losses: {'ner': 112.13581162391519}\nIteration 27, Losses: {'ner': 124.48639571322872}\nIteration 28, Losses: {'ner': 147.53310028422743}\nIteration 29, Losses: {'ner': 124.74308959723618}\nIteration 30, Losses: {'ner': 142.7312980406039}\nIteration 31, Losses: {'ner': 115.5922367782695}\nIteration 32, Losses: {'ner': 108.13337407786618}\nIteration 33, Losses: {'ner': 111.99744853618441}\nIteration 34, Losses: {'ner': 107.49273121659063}\nIteration 35, Losses: {'ner': 133.24705550609744}\nIteration 36, Losses: {'ner': 85.47288352511201}\nIteration 37, Losses: {'ner': 101.76073746626004}\nIteration 38, Losses: {'ner': 119.39595210492935}\nIteration 39, Losses: {'ner': 166.19921493680025}\nIteration 40, Losses: {'ner': 80.08354786407163}\nIteration 41, Losses: {'ner': 89.41143803694294}\nIteration 42, Losses: {'ner': 105.95224058333001}\nIteration 43, Losses: {'ner': 174.49367004839667}\nIteration 44, Losses: {'ner': 112.00063341053111}\nIteration 45, Losses: {'ner': 70.70492646543472}\nIteration 46, Losses: {'ner': 77.62390766035234}\nIteration 47, Losses: {'ner': 76.96581517051222}\nIteration 48, Losses: {'ner': 104.63152575261999}\nIteration 49, Losses: {'ner': 148.59527467687}\nIteration 50, Losses: {'ner': 90.05720303162371}\nIteration 51, Losses: {'ner': 127.72257837923196}\nIteration 52, Losses: {'ner': 115.88279837320724}\nIteration 53, Losses: {'ner': 111.08756706833127}\nIteration 54, Losses: {'ner': 98.49179335059318}\nIteration 55, Losses: {'ner': 72.52874601210569}\nIteration 56, Losses: {'ner': 141.98952555798033}\nIteration 57, Losses: {'ner': 98.59858511328956}\nIteration 58, Losses: {'ner': 92.90991052907336}\nIteration 59, Losses: {'ner': 90.51134230865883}\nIteration 60, Losses: {'ner': 89.26865266819968}\nIteration 61, Losses: {'ner': 104.62510384731245}\nIteration 62, Losses: {'ner': 100.00636556899845}\nIteration 63, Losses: {'ner': 91.90194486970525}\nIteration 64, Losses: {'ner': 67.81336827925148}\nIteration 65, Losses: {'ner': 88.15482246660352}\nIteration 66, Losses: {'ner': 119.68006967701413}\nIteration 67, Losses: {'ner': 82.75810930984397}\nIteration 68, Losses: {'ner': 66.51434179935515}\nIteration 69, Losses: {'ner': 68.28735580345489}\nIteration 70, Losses: {'ner': 62.65850502512354}\nIteration 71, Losses: {'ner': 110.77658457633459}\nIteration 72, Losses: {'ner': 76.0218293648089}\nIteration 73, Losses: {'ner': 59.58435756674384}\nIteration 74, Losses: {'ner': 62.72363212314363}\nIteration 75, Losses: {'ner': 62.570159243207826}\nIteration 76, Losses: {'ner': 78.39518403395027}\nIteration 77, Losses: {'ner': 102.93129185739986}\nIteration 78, Losses: {'ner': 75.29059192575151}\nIteration 79, Losses: {'ner': 70.01702010955786}\nIteration 80, Losses: {'ner': 77.65235412596203}\nIteration 81, Losses: {'ner': 102.5357603351448}\nIteration 82, Losses: {'ner': 105.2685009968121}\nIteration 83, Losses: {'ner': 88.08605215288348}\nIteration 84, Losses: {'ner': 98.21674553296549}\nIteration 85, Losses: {'ner': 77.3497608735404}\nIteration 86, Losses: {'ner': 72.99246872138619}\nIteration 87, Losses: {'ner': 91.44076308317095}\nIteration 88, Losses: {'ner': 108.49988186734184}\nIteration 89, Losses: {'ner': 83.32015837794347}\nIteration 90, Losses: {'ner': 99.36329199243593}\nIteration 91, Losses: {'ner': 82.30738787235288}\nIteration 92, Losses: {'ner': 70.74817394303041}\nIteration 93, Losses: {'ner': 53.63643415650218}\nIteration 94, Losses: {'ner': 57.82533007395398}\nIteration 95, Losses: {'ner': 106.6414444411242}\nIteration 96, Losses: {'ner': 75.7764326699834}\nIteration 97, Losses: {'ner': 60.448036762997994}\nIteration 98, Losses: {'ner': 66.29769533279264}\nIteration 99, Losses: {'ner': 52.21560576839632}\n","output_type":"stream"},{"name":"stderr","text":"[2024-06-07 02:12:54,147] [INFO] Created vocabulary\n[2024-06-07 02:12:54,148] [INFO] Finished initializing nlp object\n[2024-06-07 02:12:54,219] [INFO] Created vocabulary\n[2024-06-07 02:12:54,220] [INFO] Finished initializing nlp object\n","output_type":"stream"},{"name":"stdout","text":"Iteration 0, Losses: {'ner': 3598.8656812410345}\nIteration 1, Losses: {'ner': 658.3558322167391}\nIteration 2, Losses: {'ner': 403.74719275085454}\nIteration 3, Losses: {'ner': 332.9022472837606}\nIteration 4, Losses: {'ner': 285.186128007819}\nIteration 5, Losses: {'ner': 262.38933404549107}\nIteration 6, Losses: {'ner': 264.5189173134098}\nIteration 7, Losses: {'ner': 271.79522954652396}\nIteration 8, Losses: {'ner': 227.93767994499328}\nIteration 9, Losses: {'ner': 277.4646287465672}\nIteration 10, Losses: {'ner': 263.3890384696252}\nIteration 11, Losses: {'ner': 182.45044952286887}\nIteration 12, Losses: {'ner': 191.1600735884892}\nIteration 13, Losses: {'ner': 190.33478325515804}\nIteration 14, Losses: {'ner': 206.7524435575789}\nIteration 15, Losses: {'ner': 163.6204605676376}\nIteration 16, Losses: {'ner': 185.77223867502838}\nIteration 17, Losses: {'ner': 175.2221985496436}\nIteration 18, Losses: {'ner': 182.70323980150317}\nIteration 19, Losses: {'ner': 185.21900744358638}\nIteration 20, Losses: {'ner': 162.36463413709683}\nIteration 21, Losses: {'ner': 124.40515716121612}\nIteration 22, Losses: {'ner': 118.10738162096322}\nIteration 23, Losses: {'ner': 140.2309050443851}\nIteration 24, Losses: {'ner': 165.28152917314}\nIteration 25, Losses: {'ner': 164.90055227324473}\nIteration 26, Losses: {'ner': 140.30474121161816}\nIteration 27, Losses: {'ner': 129.36401299526338}\nIteration 28, Losses: {'ner': 161.17880192691857}\nIteration 29, Losses: {'ner': 129.20538286868833}\nIteration 30, Losses: {'ner': 128.45932737021502}\nIteration 31, Losses: {'ner': 112.97884240560413}\nIteration 32, Losses: {'ner': 130.23812270085514}\nIteration 33, Losses: {'ner': 121.91933448067732}\nIteration 34, Losses: {'ner': 124.16574371398562}\nIteration 35, Losses: {'ner': 140.46764804944635}\nIteration 36, Losses: {'ner': 100.50422101914926}\nIteration 37, Losses: {'ner': 107.26041960963155}\nIteration 38, Losses: {'ner': 123.76909900320493}\nIteration 39, Losses: {'ner': 119.17253093520705}\nIteration 40, Losses: {'ner': 99.20024980507925}\nIteration 41, Losses: {'ner': 97.65468877915325}\nIteration 42, Losses: {'ner': 86.65534525803065}\nIteration 43, Losses: {'ner': 88.36109830694822}\nIteration 44, Losses: {'ner': 85.17636209303525}\nIteration 45, Losses: {'ner': 117.42531686455027}\nIteration 46, Losses: {'ner': 104.74690316151967}\nIteration 47, Losses: {'ner': 86.87050165241115}\nIteration 48, Losses: {'ner': 103.09958710218856}\nIteration 49, Losses: {'ner': 89.42417223163578}\nIteration 50, Losses: {'ner': 136.65212381296448}\nIteration 51, Losses: {'ner': 86.99286251060343}\nIteration 52, Losses: {'ner': 85.31458214179573}\nIteration 53, Losses: {'ner': 121.31056711739167}\n","output_type":"stream"}]},{"cell_type":"code","source":"from spacy.scorer import Scorer\n\n# Create a scorer object\nscorer = Scorer()\n\n# Iterate over the test data\nfor text, annotations in test_data:\n    # Process the text with the trained model\n    doc = nlp(text)\n    \n    # Create an Example object\n    example = Example.from_dict(doc, annotations)\n    \n    # Update the scorer with the predictions from the model\n    scores = scorer.score([example])  # Pass a list containing the single Example object\n\n# 'scores' is now a dictionary with keys like 'ents_p', 'ents_r', 'ents_f' for entities' precision, recall, and F1-score\nprecision = scores['ents_p']\nrecall = scores['ents_r']\nf1_score = scores['ents_f']\naccuracy = scores['ents_acc']\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1-Score: {f1_score}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the Model\nmodel_path = '/kaggle/working/ner_model'\nnlp.to_disk(model_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Zip the Model Directory\nimport shutil\n\n# Create a zip file from the model directory\nshutil.make_archive(model_path, 'zip', model_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the model\nimport spacy\n\n# Load the saved model\nnlp2 = spacy.load(model_path)\n\n# Example new test data\ntest_texts = [\n\"Purchase, card *5744.  1689 RUB.  EAPTEKA.  Available 190.15 RUB\",\n\"VISA0610 15:43 Purchase 1313 rubles YANDEX EDA Balance: 317 rubles\",\n\"VISA0610 14:04 Purchase 593 RUR TATMAK Balance: 2466.14 RUR\",\n\"Payment 211.00rub Card*1835 UBER.COM Balance 6903.73rub 13:48\",\n\"Payment 211.00 rub Card*1835 UBER.COM Balance 6903.73 rub 13:48\",\n\"Payment 211.00 RUB Card*1835 UBER.COM Balance 6903.73 RUB 13:48\",\n\"Payment 123.00r Card*1835 https://taxi.ya Balance 7114.73r 21:47\",\n\"Payment 592.39 rubles Card*7293 PYATEROCHKA 519 Balance 193.73 rubles 18:43\",\n\"Transfer 2200.00r. Account*1898 Rahib f.  Balance 23.28r 13:22\",\n\"Write-off 989.00rub Card*2465 Card2Card Balance 29.74rub 19:16\",\n\"Write-off 989.00 rub Card*2465 Card2Card Balance 29.74 rub 19:16\",\n\"Withdrawal RUR 30,000.00 Card*7293 D. 17, UL.  HUSA Balance 1681.64 USD 18:47\",\n\"Deposit 300.00 USD Card*7293 D. 17, UL.  HUSA Balance 2100.05 USD 18:44\",\n\"Receipt 1800.00 USD Account*1080 from YEMEN REPUBLIC EMBASSY Balance 1800.05 USD 17:27\",\n\"Receipt 1000.00r. Account*1898 from Abdulmajid Balance 1005.58r. 08:18\",\n\"Withdrawal RUR 400.00 Card*2465 D. 84, UL.  OSTR Balance 6988.27r 05:09\",\n\"Receipt 190.00 USD Account*1080 from Abdulmalek E Balance 190.51 USD 06:44\",\n\"Transfer from AKHMED KHASAN 4800.00rub Card*2465 Balance 12201.11rub 13:46\",\n\"VISA0610 02:09 Transfer 12.15 rubles from Tinkoff Bank from RAHIB IMAD FADL N. Balance: 12.15 rubles\",\n\"VISA0610 16:05 Deposit 17,600 rub. ATM 60000722 Balance: 17,600.47 rub.\",\n\"VISA0610 10:08 Transfer 1000 rubles from Hezam A. Balance: 1000.56 rubles\",\n\"VISA0610 17:37 crediting 1500rub VTB Balance: 1547.53rub\",\n\"VISA0610 17:49 crediting 4860r VTB Balance: 5166.02r\",\n\"VISA0610 13:32 Transfer 9162r from Yana L. Balance: 9612.70r\",\n\"VISA0610 00:50 crediting 2917 rubles Tinkoff Bank Balance: 6209.64 rubles\",\n\"VISA0610 15:53   Purchase 1337 RUR KULINAR GURU BURGERS Balance: 4525.84 RUR\",\n\"VISA0610 11:54 transfer 1142r TINKOFF Balance: 9040.56r\",\n\"VISA0610 15:27 transfer 1200 rubles Balance: 12,034.56 rubles\",\n\"Replenishment, account RUB.  317 RUB.  Rahib Imad Fadl N. Available 507.15 RUB\",\n\"Replenishment, account RUB.  1000 RUB.  Khezam A. Available 1507.15 RUB\",\n\"Replenishment, account RUB.  500 RUB.  Hezam Abdulrahman Hezam A. Available 5395.15 RUB\",\n\"Translation.  RUB account.  500 RUB.  Rahib Imad Fadl N. Balance 7923.68 RUB\"\n]\n\n# Evaluate the model on each test text\nfor test_text in test_texts:\n    doc = nlp2(test_text)\n    print(f\"Text: {test_text}\")\n\n    # Initialize empty dictionary to store entities\n    entities = {\"AMOUNT\": \"\", \"CURRENCY\": \"\", \"MERCHANT\": \"\", \"CARD\": \"\", \"BALANCE\": \"\"}\n\n    # Iterate over the entities and fill the dictionary\n    for ent in doc.ents:\n        if ent.label_ in entities:\n            entities[ent.label_] = ent.text\n\n    # Print the entities\n    for label, text in entities.items():\n        if text:  # Only print if the entity was found\n            print(f\"{label} = {text}\")\n\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}